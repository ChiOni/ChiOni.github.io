---
title: BERT / Pre-training of Deep Bidrectional Transformer (2019)
date: 2022-07-17 00:00:00 +0800
categories: [Paper Review, Language]
tags: [nlp]
seo:
  date_modified: 2022-07-17 20:07:02 +0800
---



Transformer의 디코더 부분을 활용한 pre-training 모델 Bert가 19년도에 등장했다!  

- 논문 링크: [BERT: Pre-training of Deep Bidirectional Transformers for
  Language Understanding (2019)](https://arxiv.org/pdf/1810.04805.pdf)  

<br/>

<object data="/assets/data/bertpdf.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="/assets/data/bertpdf.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="/assets/data/bertpdf.pdf">Download PDF</a>.</p>
    </embed>
</object>

