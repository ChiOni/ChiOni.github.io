---
title: Why Should I Trust You? (KDD 2016)
date: 2020-01-29 00:00:00 +0800
categories: [Paper Review, Explainable]
tags: [explainable ai]
seo:
  date_modified: 2020-03-24 22:27:38 +0900
---



<br/>

딥러닝은 캡짱 멋있다. 괴물같은 정확도를 무기로 블랙 박스 모델은 여러 분야에서 기존의 휴리스틱과 통계학을 대체하고 있다. 하지만 실험실에서는 알파고 같던 딥러닝 모델들이 현실의 데이터를 마주했을 때 갑자기 멍청해지는 경우를 자주 볼 수 있다. 서비스로서 모델이 실력을 발휘해야 할 순간에 답답한 모습을 보이는 것은 두려운 일이다. 블랙박스 모델을 사용하면서도 어떻게 이런 두려움에서 벗어날 수 있을까? 바로 논문의 Motivation이 된 Trust Problem이다.  

<br/>

## <b>Trust Problem</b>  
1. 우리가 개발한 모델은 어디서든 좋은 성능을 낼 것이라 신뢰한다.  
  
2. 우리가 개발한 모델이 제시한 결과물을 신뢰한다. 

<br/>

<br/>
<img src="/assets/img/pr/lime/limeone.jpg">  
<br/>

메일의 수신자가 기독교인지 무교인지 구별하는 과제가 있다. NLP Task 중 Text Classification에 속하는 문제가 있다고 한다. 아직 논문이 제안하는 모델이 무엇인지 살펴보지는 않았지만 우리는 딥러닝의 의사결정을 이해할 수 있다고 한다. 그런데 Train Set에서 높은 정확도를 보이는 우리의 모델이 의사결정에 중요하게 생각하는 것이 발신자의 이름이라고 한다. 당연한 소리지만 현실 세계에서는 무한히 다양한 발신자가 존재한다. <b> 즉, 현실 세계의 데이터에서 발신자의 이름을 중요 Feature로 채택한 위의 모델은 실패할 것이다.</b>    

<br/>
<img src="/assets/img/pr/lime/limetwo.jpg">  
<br/>

- Flu가 정답이라는 근거는 sneeze / headache  
  
- Flu가 정답이 아닐 수 있는 이유는 no fatigue  
  

환자의 기록을 통해 증상을 예측하는 모델의 결과물을 의사는 어떻게 신뢰할 수 있을까? 만약 모델이 친절하게도 우리가 이해할 수 있는 수준의 이유를 알려준다면 사용자는 모델을 신뢰할 수 있을 것이다. 논문에서 추구하는 "신뢰를 바탕한 의사결정" 은 쉬운 설명(?)을 바탕으로 한다.  

<br/>

<b>기존에 연구자들은 ML 모델에 납득가능한 신뢰를 얻기 위해 아래의 방법들을 사용해왔다.</b>  
<b>1. Interpretable Model </b>(결과물을 해석할 수 있는 얕은 tree model 등을 사용)
<br>
<b>2. Accuracy </b> (실험의 스코어를 신뢰) 
<br>
<b>3. A/B Testing </b> (가설에 대한 현실 유저의 피드백)  

<br/>
그러나 위의 3가지 방법들은 성능이 좋지 않거나, 현실에서 적용되지 못하거나, 비용이 많이 드는 등의 문제가 존재했다. 그리고 이런 문제점들을 극복하고자 논문에서 저자가 제시한 모델이 바로 <b>LIME (Locally Interpretable Model-Agnostic)이다.</b> 이름만으로 모델을 해석해보자면 LIME은 Global하게 일반화할 수 없는 지협적인 설명을(Locally), 인간이 이해할 수 있는 수준의 방법으로(Interpretable) 제시하는데, 이것을 어떠한 Black-Box 모델에도 확장하여(model-agnostic) 설명(Explanaible)할 수 있다. 오! 무슨 소린지 하나도 모르겠다.   
<br/>
<img src="/assets/img/pr/lime/limethree.jpg">  
<br/>

2차원의 한 점을 찍어 그 점이 빨강인지 파랑인지 분류하는 문제를 생각해보자. 모델은 굵은 십자가를 빨강으로 예측했다. 그러나 우리는 의사 결정의 기준이 되는 구불구불한 곡선이 어떤 수식으로 생겨먹은지 이해할 수 없다. 그럼 저 곡선말고 그림의 검은 점선은 어떤가. 굵은 십자가 근처에서의 구불구불 곡선의 의사결정은 아주 간단한 점선과 같은 결정을 내리고 있다. LIME은 점선을 찾는 모듈이다. <b>복잡한 의사결정을 흉내내는 쉬운 모델을 개발하는 것이라 볼 수 있다.</b>  
<br/>

## <b>LIME을 구현하는 4단계</b>  
1. Input을 간단한 형태로 가공한다. 

  = <b>x를 x'로 가공한다.  </b>
  <br>

2. 가공된 Input과 비슷한 데이터를 창조(sampling)한다. 

  = <b>x'와 유사한 z' 를 만든다.  </b>
  <br>

3. 창조된 데이터를 복잡한 형태로 가공한다. 

  = <b>z'를 z로 가공한다.  </b>
  <br>

4. 복잡한 모델과 유사한 결과물을 내는 간단한 모델을 만든다. 

  = <b>f(z) = g(z')인 g를 찾는다.</b>  
  <br/>

하나 하나의 과정을 조금 자세하게 풀어보자면  
<br>
<b>1. Input을 간단한 형태로 가공한다.</b>  

상식적으로 복잡한 데이터에 (예를 들면 64 x 64 x 3의 이미지) 대한 분류를 간단히 설명하기는 쉽지 않을 것이다. 따라서 이 복잡하고 커다란 데이터를 0과 1로만 이루어진 단순한 덩어리로 바꿔주는 작업이 필요하다.  

예를들어 데이터가 텍스트라면 word embedding한 값이 아니라 그냥 단어 덩어리를 인풋으로, 데이터가 이미지라면 하나 하나의 픽셀을 다 넣는 것이 아니라 super pixel 등의 덩어리를 인풋으로 치환해주는 방식으로 Input을 가공해준다.  
<br/>

<b>2. 가공된 Input과 비슷한 데이터를 창조(sampling)한다.</b>  

우리의 복잡한 이미지가 이제 Step1을 통해 x' = (1,1,1,1,1)와 같은 단순한 벡터로 치환되었다.  

그렇다면 samping(?)을 통해 z'들을 만들어낸다.  
- z' = [z'(1), z'(2), z'(3)] = [(1,1,1,1,0), (1,1,1,0,1), (1,1,1,0,0)]
- z'(1)보다는 z'(3)이 x'와 더 멀리 있는 샘플이다.  
<br/>

<b>3. 창조된 데이터를 복잡한 형태로 가공한다.</b>  

z'를 z로 바꿔주는 과정. 설명의 편의를 위해 x를 이미지라고 가정한다.  

z(1)은 x'에서 5번째 슈퍼픽셀이 있었어야 할 부분이 회색으로 칠해진 이미지가 된다.  

z(3)은 3번째 원소와 5번째 슈퍼픽셀이 있었어야 할 부분이 회색으로 칠해진 이미지가 된다.  

복잡한 모델 f가 z(1)을 분류할때는 회색 영역이 없는 기존의 z와 다른 결과물을 낼 것이다.  
<br/>

<b>4. 복잡한 모델과 유사한 결과물을 내는 간단한 모델을 만든다.</b>  

<img src="/assets/img/pr/lime/limefour.jpg">  
쉬운 모델의 학습을 이해하기 위한 유일한 수식이다. 모델은 학습을 거듭하며 f(z) = g(z')이 되도록 진화한다. 여기서 기존 L2 loss와 조금 다른 점은 바로 저 앞에 붙은 가중치이다. 멀리 있는 sample은 loss에 영향을 조금 끼치도록 거리가 inverse exponential하게 가중된다. 앞서 우리는 x'를 길이 5의 벡터로 가정했기 때문에 모델 g의 피쳐는 5개이다. 그럼 우리는 학습이 완료된 시점에, 5개의 피쳐 중 어떤 것이 혹은 몇 개가 중요한지에 대해 Lasso를 통해 추출할 수 있다. 만약 3번째가 가장 중요한 피쳐라면, 이미지의 분류에 가장 중요한 곳은 3번째 슈퍼픽셀이 있는 부분일 것이다.  
<br/>
<img src="/assets/img/pr/lime/limefive.jpg">  
<br/>
아직도 납득이 가지 않은 나를 위한 논문에서 제공한 아주 직관적인 예시가 있다. 이미지 분류 모델은 맨 왼쪽의 원본 사진을 보고 (전자 기타 > 통기타 > 강아지) 세 가지 정답을 제시했다. 이 모델을 LIME으로 근사하고, 각 정답이 유도되는데 이미지의 어떤 덩어리가 중요했는지 역으로 있다. 만약 전자기타를 정답으로 예측해놓고 중요한 부분은 강아지 얼굴이라고 제시했다면 우리는 모델을 신뢰할 수 없을 것이다. 그러나 보다싶이 모델이 아주 그럴듯한 부분을 중요하다고 콕 찝어줬다는 것을 볼 수 있다.
<br/>

<br/>
[(PAPER URL)why should i trust you explaining the predictions of any classifier](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)   
