---
title: Attention is all you need (2017)
date: 2022-07-07 00:00:00 +0800
categories: [Paper Review, Language]
tags: [nlp]
seo:
  date_modified: 2022-07-07 20:07:02 +0800
---







구글에서 2017년에 Transformer 구조를 제안했고, 그 효과는 아주아주 대단했다.  

논문의 핵심 컨셉인 self-attention을 활용한 파생 논문들을 앞으로 쭉 보고,  

어딘가에 implement하는 방식으로 스터디를 진행 할 계획이다.  

- 논문 링크: [Attention is all you need (2017)](https://arxiv.org/pdf/1706.03762.pdf)  

<br/>

<object data="/assets/data/attentionisallyouneed.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="/assets/data/attentionisallyouneed.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="/assets/data/attentionisallyouneed.pdf">Download PDF</a>.</p>
    </embed>
</object>

